{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Multiple Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essentially, we just give a file path instead of a file name!\n",
    "\n",
    "# We first need to import Python's built-in OS library, which allows our code to find and access the names of the files in the folder you specify\n",
    "import os\n",
    "\n",
    "# ./ is saying that the folder, antiquities_reports, is in the same directory as this notebook\n",
    "folder_path = './antiquities_reports'\n",
    "\n",
    "list_of_reports = []\n",
    "\n",
    "# os.listdir is like the \"cd\" command, creating a list of each file in the specified folder\n",
    "for file in os.listdir(folder_path):\n",
    "    # we do this check bc some OS include \"hidden\" files that could trip your code up like .DS_Store on MacOS\n",
    "    if file.endswith('.txt'):\n",
    "        # we must include the full path to the file with the folder name, so the code looks for the file in the right place rather than just in the folder this notebook is in\n",
    "        full_path = folder_path + '/' + file\n",
    "        full_text = open(full_path, encoding=\"utf-8\").read()\n",
    "        list_of_reports.append(full_text)\n",
    "\n",
    "list_of_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you could also store the files in a dictionary format, if the file name is relevant to your work\n",
    "\n",
    "dict_of_reports = {}\n",
    "\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith('.txt'):\n",
    "        full_path = folder_path + '/' + file\n",
    "        full_text = open(full_path, encoding=\"utf-8\").read()\n",
    "        # to add to a dictionary, you simply state a UNIQUE key (in our case, the file name) in square brackets, and the value is placed after the assignment operator\n",
    "        dict_of_reports[file] = full_text\n",
    "\n",
    "dict_of_reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PDFs\n",
    "\n",
    "There are multiple libraries available to import PDFs into Python; in this instance, I am using `pdfplumber`. It has [a GitHub page with extensive documentation on how to use it](https://github.com/jsvine/pdfplumber?tab=readme-ov-file#python-library)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "# this creates a \"PDF\" object, essentially a data type custom to this library\n",
    "pdf = pdfplumber.open('./MRE-mitm.pdf')\n",
    "\n",
    "complete_pdf = ''\n",
    "# pdf.pages gets a list of pages from our PDF object, in the format of a \"Page\" object, a subtype of a PDF object with page-level information\n",
    "# \"enumerate()\" is a built-in Python function that allows us to get both the item in the list AND its index, which in this case represents the page number\n",
    "for index, page in enumerate(pdf.pages):\n",
    "    # get page data for page #x\n",
    "    curr_page = pdf.pages[index]\n",
    "    # extract the text from that page\n",
    "    text = curr_page.extract_text()\n",
    "    complete_pdf += text\n",
    "\n",
    "print(complete_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: This method works for PDFs where the text is searchable (aka PDFs that have already been OCRed), which is most PDFs since most PDF creation software does this for automatically for printed text. If you have an image-based PDF that contains printed text without existing OCR, the process to use these with Python is more complicated-- first, you would convert the PDF pages into individual images using a library like `pdf2image`, then OCR each of these images using the library `pytesseract`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization\n",
    "\n",
    "Like with PDFs, Python also has multiple data visualization libraries. I prefer Plotly, since it produces interactive visuals that can be saved as an HTML file which can be embedded like a video or image. It also comes with built in \"testing\" data you can experiment with, and has almost any graph you could imagine with clear example code! [Check it out](https://plotly.com/python/)! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an implementation of a standard [donut chart](https://plotly.com/python/pie-charts/#donut-chart):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# some dummy data\n",
    "labels = ['Oxygen','Hydrogen','Carbon_Dioxide','Nitrogen']\n",
    "values = [4500, 2500, 1053, 500]\n",
    "\n",
    "# we create a plotly \"Figure\", which is plotly's graph output type\n",
    "# the hole value indicates the size of the hole-- you could omit it to make a pie chart\n",
    "fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But what about more complex data? Most of the time, our data is not neatly tallied up like this... so we use pandas to clean things up!\n",
    "# Let's transform our socioecstatus data from the witches dataset into a donut chart as an example of doing this\n",
    "\n",
    "# hello again, counter!\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "witches_df = pd.read_csv(\"wdb_accused.csv\",  delimiter=\",\") \n",
    "witches_df = witches_df.fillna(\"Unknown\")\n",
    "\n",
    "witches_socioecstatus = list(witches_df['socioecstatus'])\n",
    "\n",
    "# we can use Counter to tally up the items in our list\n",
    "socioecstatus_tally = Counter(witches_socioecstatus).most_common()\n",
    "\n",
    "# Counter returns a list of tuples, which is an odd data type that can be difficult to work with, so let convert our results to a dictionary\n",
    "socioecstatus_dict = dict(socioecstatus_tally)\n",
    "\n",
    "# and I know from looking at that dict the \"Unknown\" is disproportionately represented, so I'm going to delete that particular key to make our graph more readable\n",
    "del socioecstatus_dict[\"Unknown\"]\n",
    "\n",
    "socioecstatus_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can cast the keys and values results from the dict into a list\n",
    "fig = go.Figure(data=[go.Pie(labels=list(socioecstatus_dict.keys()), values=list(socioecstatus_dict.values()), hole=.5)])\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping Examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to be a Good Webscraper (aka don't get blacklisted!)\n",
    "\n",
    "First and foremost, ensure that there is no clause on the website(s) you're planning to scrape that expilicitly state that it cannot be legally scraped and/or using the information provided by the website outside of it is a violation (this information can often be found in the Terms of Service). \n",
    "\n",
    "If that is clear, then essentially, to be an ethical webscraper you want your scraping process to imitate a human interacting with the website as much as possible, since that is how most websites are designed to be used! There are a few main ways this can be achieved:\n",
    "- **Wait times**: If you scrape a website too aggressively-- for example, asking a website for information or a new link very quickly one after another-- you risk overloading their servers (particularly an issue for older or smaller/more niche websites) which can cause the site to crash, or more likely, you to be blacklisted from accessing the website because you will be flagged as malicious/a bot. To combat this, we can add small wait times between our requests for information; these pauses give the servers a break!  \n",
    "- **Automate actions**: If possible, consider how you would interact with the webpage you want to scrape, and try imitating those steps through your code (discussed further in \"Selenium\").\n",
    "- **Use \"Inspect Element\" to analyse HTML**: Rather than request the entire HTML content of a webpage in your code to find the information you want to extract (hence, making more requests to their servers), look at the webpage's HTML through your browser's \"Inspect\" or \"Inspect Element\" tool!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup\n",
    "\n",
    "BeautifulSoup is a simple Python library that allows you to read the HTML content of a provided webpage into your code, allowing you to extract information present in it.\n",
    "Here are some examples of how the HTML of your page can be searched through and extracted from: https://beautiful-soup-4.readthedocs.io/en/latest/index.html#searching-the-tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping articles from news website\n",
    "\n",
    "# import two Python libraries:\n",
    "# requests allows you to request information from a website's server\n",
    "# sleep allows use to pause our code to give the servers a break\n",
    "import requests\n",
    "from time import sleep\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# let's just get the first 3pgs of results from our search for now --> the following for loop imitates clicking \"next page\"\n",
    "article_links = []\n",
    "for i in range(1, 4):\n",
    "    # the URL we are requesting information from\n",
    "    url = \"https://betakit.com/query/fintech/page/\" + str(i)\n",
    "\n",
    "    # requesting the information --> \"user agent\" is telling the server to respond as if the request was coming from a Mozilla (Firefox) browser\n",
    "    req = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "    # now we take a brief pause before continuing to manipulate the requested data\n",
    "    sleep(5)\n",
    "    html = req.text\n",
    "\n",
    "    # now we use BeautifulSoup to parse the extract HTML and create a \"soup\" object that can be used to search and find the exact information we want\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "\n",
    "    # to extract the link to each article, I found the HTML element using the \"Inspect\" tool and I am selecting all results which match that element using CSS\n",
    "    links = soup.select(\"h2.entry-title > a\")\n",
    "\n",
    "    # now, I am adding each link I extracted to a list that will be ongoing, capturing the article links from all 15pgs\n",
    "    for a in links:\n",
    "        article_links.append(a['href'])\n",
    "\n",
    "    print(\"On page \" + str(i) + \"! Sleeping to next page...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view list of article links --> there are 15 articles on each page so 15*15 means I should have 225 links\n",
    "article_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can extract some metadata and content of each article and organize this information to a dataframe\n",
    "\n",
    "# create an empty dataframe with just the column headers\n",
    "articles_df = pd.DataFrame(columns=['author', 'date', 'title', 'content', 'tags'])\n",
    "\n",
    "for article in article_links:\n",
    "    # request information from the article webpage, just like we did before with the search page\n",
    "    url = article\n",
    "    req = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    html = req.text\n",
    "\n",
    "    sleep(10)\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "\n",
    "    # more information on the search + extract from HTML methods provided by BeautifulSoup in the link above\n",
    "    author = soup.find(\"a\", \"author url fn\").get_text()\n",
    "    date = soup.find(\"span\", \"entry-date\").get_text()\n",
    "    article_title = soup.find(\"h1\", \"entry-title\").get_text().strip()\n",
    "    article_content = soup.select(\"article > p\")[0].get_text().strip()\n",
    "    tags = soup.select(\"div#tags-box > a\")\n",
    "\n",
    "    # put info extracted from article into a list\n",
    "    new_article_row = [author, date, article_title, article_content, tags]\n",
    "    # add list to our dataframe as new row\n",
    "    articles_df.loc[len(articles_df)] = new_article_row  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view our completed dataframe!\n",
    "articles_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selenium\n",
    "\n",
    "[Selenium](https://selenium-python.readthedocs.io/) is a more advanced tool designed for automating web browsers through the use of **WebDrivers**; this essentially means the Selenium creates its own browser window from the browser you tell it to use, and in this window it performs tasks which you tell it to do through your code. This does mean that you shouldn't interact with the browser window Selenium creates, unless you want to risk messing up its scraping process! \n",
    "\n",
    "Selenium is often used in conjunction with BeautifulSoup, where Selenium performs an action that is necessary to make the information you need appear, and once that information has appeared you can use BeautifulSoup as usual.\n",
    "\n",
    "In the following example, the webpage requires the user to search a location in a text box in order for the information on that location to appear, which necessitates the use of Selenium to automate scraping information from this site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example locations to search\n",
    "municipalities = ['Capixaba - AC',\n",
    "                'Cruzeiro Do Sul - AC',\n",
    "                'Cacimbinhas - AL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from time import sleep\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will select the Chrome WebDriver for Selenium to use in its window\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# create an empty dataframe with just the column headers\n",
    "descargas_df = pd.DataFrame(columns=['cidade/uf', 'densidade_descargas', 'rank_densidade_n', 'rank_densidade_e'])\n",
    "\n",
    "for m in municipalities:\n",
    "    # we want to open this webpage with our WebDriver in order interact with it \n",
    "    driver.get(\"http://www.inpe.br/webelat/homepage/#\")\n",
    "    \n",
    "    # Through the \"Inspect\" tool (in my own browser window, not the Selenium generated one!) I identify the HTML element which Selenium needs to interact with\n",
    "    # an input text field\n",
    "    inputElement = driver.find_element(\"id\", \"input_ranking\")\n",
    "    # the send_keys(m) command will enter the current selected municipality into the input text field identified in the previous line of code\n",
    "    inputElement.send_keys(m)\n",
    "    # this simulated you hitting the \"enter\" key and reveals the information we want\n",
    "    inputElement.send_keys(Keys.ENTER)\n",
    "\n",
    "    sleep(5)\n",
    "    soup = BeautifulSoup(driver.page_source,\"html.parser\")\n",
    "    \n",
    "    # now we can extract the information from the HTML element that was uncovered after searching for it using Selenium\n",
    "    scraped_tbl = soup.find(id=\"divRanking\").get_text()\n",
    "    # since it is extracted from HTML, this information is in an odd format so we'll put it into a list to make it more usable\n",
    "    scraped_tbl = scraped_tbl.split('\\n')\n",
    "    \n",
    "    # we just want the information following the \":\" in each extracted row of information\n",
    "    densidade_info = []\n",
    "    for i in scraped_tbl:\n",
    "        if len(i) > 1 and ':' in i:\n",
    "            i = i.strip().split(':')[1]\n",
    "            densidade_info.append(i)\n",
    "\n",
    "    print(densidade_info)\n",
    "\n",
    "    # now we can add the list of infomation as a new row in our dataframe\n",
    "    descargas_df.loc[len(descargas_df)] = densidade_info  \n",
    "    print('-------')\n",
    "\n",
    "# this closes the Selenium window\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view your completed dataframe!\n",
    "descargas_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
